\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[]{neurips_2019}
\usepackage{project_440_550}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage[dvipsnames]{xcolor}
\usepackage[normalem]{ulem}
\newif{\ifhidecomments}


\title{[Re] Diffusion-Based Adversarial Sample Generation for Improved Stealthiness and Controllability}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  William Kang,
  \\
  \texttt{syu@student.ubc.ca}
  \And
  Christina Yang\\
  \texttt{chryang@student.ubc.ca}
}

\begin{document}

\maketitle

\begin{abstract}
  We are doing a reproducibility report based on the paper "Diffusion-Based Adversarial Sample Generation for Improved Stealthiness and Controllability" by Haotian Xue, Alexandre Araujo, Bin Hu, Yongxin Chen. Their Github Repo is here: \url{https://github.com/xavihart/Diff-PGD/tree/main}.

  The paper uses a novel framework to generate adversarial samples. They use a gradient based method guided by a pre-trained diffusion model to try to generate images that appear realistic to the human eye, can fool a wide range of models, and is easy to control how certain regions are modified.

  In particular, we are evaluating the claims that Diff-PGD outperform baseline methods such as PGD, AdvPatch, and AdvCam in physical-world attacks and style-based attacks specifically.
  Finally, we are evaluating the claim that Diff-PGD generates adversarial samples with higher stealthiness.
\end{abstract}


\section*{\centering Reproducibility Summary}

\textit{Template and style guide to \href{https://paperswithcode.com/rc2020}{ML Reproducibility Challenge 2020}. The following section of Reproducibility Summary is \textbf{mandatory}. This summary \textbf{must fit} in the first page, no exception will be allowed. When submitting your report in OpenReview, copy the entire summary and paste it in the abstract input field, where the sections must be separated with a blank line.
}

\subsection*{Scope of Reproducibility}

State the main claim(s) of the original paper you are trying to reproduce (typically the main claim(s) of the paper).
This is meant to place the work in context, and to tell a reader the objective of the reproduction.

\subsection*{Methodology}

Briefly describe what you did and which resources you used. For example, did you use author's code? Did you re-implement parts of the pipeline? You can also use this space to list the hardware used, and the total budget (e.g. GPU hours) for the experiments.

\subsection*{Results}

Start with your overall conclusion --- where did your results reproduce the original paper, and where did your results differ? Be specific and use precise language, e.g. "we reproduced the accuracy to within 1\% of reported value, which supports the paper's conclusion that it outperforms the baselines". Getting exactly the same number is in most cases infeasible, so you'll need to use your judgement to decide if your results support the original claim of the paper.

\subsection*{What was easy}

Describe which parts of your reproduction study were easy. For example, was it easy to run the author's code, or easy to re-implement their method based on the description in the paper? The goal of this section is to summarize to a reader which parts of the original paper they could easily apply to their problem.

\subsection*{What was difficult}

Describe which parts of your reproduction study were difficult or took much more time than you expected. Perhaps the data was not available and you couldn't verify some experiments, or the author's code was broken and had to be debugged first. Or, perhaps some experiments just take too much time/resources to run and you couldn't verify them. The purpose of this section is to indicate to the reader which parts of the original paper are either difficult to re-use, or require a significant amount of work and resources to verify.

\subsection*{Communication with original authors}

Briefly describe how much contact you had with the original authors (if any).
\newpage
\textit{\textbf{The following section formatting is \textbf{optional}, you can also define sections as you deem fit.
\\
Focus on what future researchers or practitioners would find useful for reproducing or building upon the paper you choose.}}
\section{Introduction}
A few sentences placing the work in high-level context. Limit it to a few paragraphs at most; your report is on reproducing a piece of work, you donâ€™t have to motivate that work.

\section{Scope of reproducibility}
\label{sec:claims}

Introduce the specific setting or problem addressed in this work, and list the main claims from the original paper. Think of this as writing out the main contributions of the original paper. Each claim should be relatively concise; some papers may not clearly list their claims, and one must formulate them in terms of the presented experiments. (For those familiar, these claims are roughly the scientific hypotheses evaluated in the original work.)

A claim should be something that can be supported or rejected by your data. An example is, ``Finetuning pretrained BERT on dataset X will have higher accuracy than an LSTM trained with GloVe embeddings.''
This is concise, and is something that can be supported by experiments.
An example of a claim that is too vague, which can't be supported by experiments, is ``Contextual embedding models have shown strong performance on a number of tasks. We will run experiments evaluating two types of contextual embedding models on datasets X, Y, and Z."

% This section roughly tells a reader what to expect in the rest of the report. Clearly itemize the claims you are testing:

We investigate the main claims from the original paper, which are:
\begin{enumerate}
    % \item Diffusion-Based Projected Gradient Descent (Diff-PGD) generates realistic adversarial samples
    \item Diff-PGD can be applied to specific tasks such as digital attacks, physical-world attacks, and style-based attacks, outperforming baseline methods such as PGD, AdvPatch, and AdvCam.
    \item Diff-PGD is more stable and controllable compared to existing methods for generating natural-style adversarial samples.
    % \item The samples generated using Diff-PGD have better transferability and anti-purification power than traditional gradient-based methods.
    \item Diff-PGD surpasses the original PGD in Transferability and Purification power
    \item Diff-PGD generates adversarial samples with higher stealthiness
    % \item Diff-PGD generates images that are more stealthy compared to PGD-generated samples for digital attacks.
    % \item Diff-rPGD generates images that are more stealthy compared to rPGD-generated samples for digital attacks.
\end{enumerate}

Each experiment in Section~\ref{sec:results} will support (at least) one of these claims, so a reader of your report should be able to separately understand the \emph{claims} and the \emph{evidence} that supports them.

%\jdcomment{To organizers: I asked my students to connect the main claims and the experiments that supported them. For example, in this list above they could have ``Claim 1, which is supported by Experiment 1 in Figure 1.'' The benefit was that this caused the students to think about what their experiments were showing (as opposed to blindly rerunning each experiment and not considering how it fit into the overall story), but honestly it seemed hard for the students to understand what I was asking for.}

\section{Methodology}
Explain your approach - did you use the author's code, or did you aim to re-implement the approach from the description in the paper? Summarize the resources (code, documentation, GPUs) that you used.

\subsection{Model descriptions}
Include a description of each model or algorithm used. Be sure to list the type of model, the number of parameters, and other relevant info (e.g. if it's pretrained).

\subsection{Datasets}
For each dataset include 1) relevant statistics such as the number of examples and label distributions, 2) details of train / dev / test splits, 3) an explanation of any preprocessing done, and 4) a link to download the data (if available).

The original paper used ImageNet, but due to limitations in compute and memory, we used a smaller subset of ImageNet with 1000 samples: \url{https://www.kaggle.com/datasets/ifigotin/imagenetmini-1000}

\subsection{Hyperparameters}
Describe how the hyperparameter values were set. If there was a hyperparameter search done, be sure to include the range of hyperparameters searched over, the method used to search (e.g. manual search, random search, Bayesian optimization, etc.), and the best hyperparameters found. Include the number of total experiments (e.g. hyperparameter trials). You can also include all results from that search (not just the best-found results).

\subsection{Experimental setup and code}
Include a description of how the experiments were set up that's clear enough a reader could replicate the setup.
Include a description of the specific measure used to evaluate the experiments (e.g. accuracy, precision@K, BLEU score, etc.).
Provide a link to your code.

We ran the code given by the authors.
We copied the hyperparameter setup of the authors.\\

Physical-World Attacks:\\
We first tried the one of the attacks created by the authors, which was a computer-mouse.\\
We then tried our own physical world attack using an image patch of a ... and a ... as our target object.\\
We use an (type of phone here, ex. iPhone 8-Plus) to take images from the real world and use an (type of printer here, ex. HP DeskJet-2752) to print the image in color.\\
We stuck the original image on ..., and classified it using all 5 classifiers (R50, R101, R18, WR50, WR101)

We tested for Success Attack Rate of Diff-PGD using 250 uniformly sampled images from our dataset. (See figure ...)\\
The code for the figure in the paper was not provided, so we created our own code to generate the figure.\\

We also need to generate anti-purification table from paper, but I'm not sure how to generate this.

Transferability: Figure 6b+6c\\
We also test the success rate attacking adversarially trained ResNet-50

\subsection{Computational requirements}
Include a description of the hardware used, such as the GPU or CPU the experiments were run on.
For each model, include a measure of the average runtime (e.g. average time to predict labels for a given validation set with a particular batch size).
For each experiment, include the total computational requirements (e.g. the total GPU hours spent).
(Note: you'll likely have to record this as you run your experiments, so it's better to think about it ahead of time). Generally, consider the perspective of a reader who wants to use the approach described in the paper --- list what they would find useful.

\section{Results}
\label{sec:results}
Start with a high-level overview of your results. Do your results support the main claims of the original paper? Keep this section as factual and precise as possible, reserve your judgement and discussion points for the next "Discussion" section.
\begin{center}
  Original paper results
  \begin{tabular}{ l c c c c r }
    \hline
    \textbf{Sample} & \textbf{(+P)ResNet50} & \textbf{(+P)ResNet101} & \textbf{(+P)ResNet18} & \textbf{(+P)WRN50} & \textbf{(+P)WRN101} \\ \hline
    $x_{PGD}$ & 0.35 & 0.18 & 0.26 & 0.20 & 0.17 \\
    $x_{n}$ (Ours) & 0.35 & 0.18 & 0.26 & 0.20 & 0.17 \\
    $x_{n}^0$ (Ours) & 0.35 & 0.18 & 0.26 & 0.20 & 0.17 \\
    \hline
  \end{tabular}
\end{center}



\subsection{Results reproducing original paper}
For each experiment, say 1) which claim in Section~\ref{sec:claims} it supports, and 2) if it successfully reproduced the associated experiment in the original paper.
For example, an experiment training and evaluating a model on a dataset may support a claim that that model outperforms some baseline.
Logically group related results into sections.

\subsubsection{Result 1}

\subsubsection{Result 2}

\subsection{Results beyond original paper}
Often papers don't include enough information to fully specify their experiments, so some additional experimentation may be necessary. For example, it might be the case that batch size was not specified, and so different batch sizes need to be evaluated to reproduce the original results. Include the results of any additional experiments here. Note: this won't be necessary for all reproductions.

\subsubsection{Additional Result 1}
\subsubsection{Additional Result 2}

\section{Discussion}

Give your judgement on if your experimental results support the claims of the paper. Discuss the strengths and weaknesses of your approach - perhaps you didn't have time to run all the experiments, or perhaps you did additional experiments that further strengthened the claims in the paper.

\subsection{What was easy}
Give your judgement of what was easy to reproduce. Perhaps the author's code is clearly written and easy to run, so it was easy to verify the majority of original claims. Or, the explanation in the paper was really easy to follow and put into code.

Be careful not to give sweeping generalizations. Something that is easy for you might be difficult to others. Put what was easy in context and explain why it was easy (e.g. code had extensive API documentation and a lot of examples that matched experiments in papers).

\subsection{What was difficult}
List part of the reproduction study that took more time than you anticipated or you felt were difficult.

Be careful to put your discussion in context. For example, don't say "the maths was difficult to follow", say "the math requires advanced knowledge of calculus to follow".

\subsection{Communication with original authors}
Document the extent of (or lack of) communication with the original authors. To make sure the reproducibility report is a fair assessment of the original research we recommend getting in touch with the original authors. You can ask authors specific questions, or if you don't have any questions you can send them the full report to get their feedback before it gets published.



\section*{References}


\end{document}
